{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef10fc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\iremm\\.conda\\envs\\BTK-AI\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\iremm\\.cache\\huggingface\\hub\\datasets--sajjadhadi--disease-diagnosis-dataset. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating train split: 100%|██████████| 469195/469195 [00:01<00:00, 462793.49 examples/s]\n",
      "Generating test split: 100%|██████████| 24695/24695 [00:00<00:00, 1643239.85 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['diagnosis', 'text'],\n",
      "        num_rows: 469195\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['diagnosis', 'text'],\n",
      "        num_rows: 24695\n",
      "    })\n",
      "})\n",
      "{'diagnosis': 'personality disorder', 'text': 'Patient reported these symptoms: depression ,hostile behavior ,drug abuse ,delusions or hallucinations ,temper problems ,fears and phobias and low self-esteem  may indicate. based on these symptoms, what disease may the patient have? The patient may have personality disorder.'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Verisetini indirme\n",
    "dataset = load_dataset(\"sajjadhadi/disease-diagnosis-dataset\")\n",
    "\n",
    "# Verinin örneklerine göz atma\n",
    "print(dataset)\n",
    "\n",
    "# Eğitim setinin ilk örneğine bakma\n",
    "print(dataset['train'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f9acb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 469195/469195 [00:17<00:00, 26391.94 examples/s]\n",
      "Map: 100%|██████████| 24695/24695 [00:01<00:00, 21249.69 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'diagnosis': 'personality disorder', 'text': 'depression ,hostile behavior ,drug abuse ,delusions or hallucinations ,temper problems ,fears and phobias and low self-esteem  may indicate.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Metin temizleme fonksiyonu\n",
    "def clean_text(text):\n",
    "    # Gereksiz başlangıç kısmını ve son kısmını temizle\n",
    "    text = re.sub(r\"Patient reported these symptoms: \", \"\", text)\n",
    "    text = re.sub(r\" based on these symptoms, what disease may the patient have\\? The patient may have .+\", \"\", text)\n",
    "    return text.strip()\n",
    "\n",
    "# Eğitim verisindeki 'text' bölümünü temizleme\n",
    "dataset['train'] = dataset['train'].map(lambda x: {'text': clean_text(x['text']), 'diagnosis': x['diagnosis']})\n",
    "\n",
    "# Test verisindeki 'text' bölümünü temizleme\n",
    "dataset['test'] = dataset['test'].map(lambda x: {'text': clean_text(x['text']), 'diagnosis': x['diagnosis']})\n",
    "\n",
    "# Temizlenmiş veriyi kontrol etme\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e05f1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\iremm\\.conda\\envs\\BTK-AI\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\iremm\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-en-trk. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nMarianTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHelsinki-NLP/opus-mt-en-trk\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# İngilizce'den Türkçe'ye çeviri modeli\u001b[39;00m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m MarianMTModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m----> 6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mMarianTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(model_name)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Çeviri fonksiyonu\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranslate\u001b[39m(text):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Metni tokenlara çevirme\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\utils\\import_utils.py:1690\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1688\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1689\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1690\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\utils\\import_utils.py:1678\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1676\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m-> 1678\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[1;31mImportError\u001b[0m: \nMarianTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Model ve tokenizer'ı yükleme\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-trk'  # İngilizce'den Türkçe'ye çeviri modeli\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Çeviri fonksiyonu\n",
    "def translate(text):\n",
    "    # Metni tokenlara çevirme\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    # Modeli kullanarak çeviri yapma\n",
    "    translated = model.generate(**tokens)\n",
    "    # Çevrilen metni çözme\n",
    "    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "# Eğitim verisindeki 'text' bölümünü Türkçeye çevirme\n",
    "dataset['train'] = dataset['train'].map(lambda x: {'text': translate(x['text']), 'diagnosis': x['diagnosis']})\n",
    "\n",
    "# Test verisindeki 'text' bölümünü Türkçeye çevirme\n",
    "dataset['test'] = dataset['test'].map(lambda x: {'text': translate(x['text']), 'diagnosis': x['diagnosis']})\n",
    "\n",
    "# Çevrilmiş veriyi kontrol etme\n",
    "print(dataset['train'][0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BTK-AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
